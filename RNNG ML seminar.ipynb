{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network Grammars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this notebook\n",
    "\n",
    "These notes were created as a presentation aid for the paper [Recurrent Neural Network Grammars](https://arxiv.org/abs/1602.07776), Dyer et al. 2016, at the [Berlin Machine Learning seminar](http://building-babylon.net/berlin-machine-learning-learning-group/). All images are from Dyer et al. 2016. For more information about code sources and licensing, please see the project README. All mistakes are, naturally, my own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Grammar\n",
    "\n",
    "Traditionally defined:\n",
    "\n",
    "<!-- G = (N, Sig, P, S)-->\n",
    "\n",
    "<!-- img src=\"img/grammar.png\",width=200,height=200 --->\n",
    "\n",
    "\\begin{align}\n",
    "G = (N, \\Sigma, P, <.>)\n",
    "\\end{align}\n",
    "\n",
    "* N: set of nonterminals\n",
    "* $\\Sigma$: set of terminals (words)\n",
    "* P: set of production rules\n",
    "* <.>: start symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Mini-Generative-Grammar\n",
    "\n",
    "1. <.> -> S\n",
    "* S -> NP VP \n",
    "* NP -> N | DET N | DET ADJ N | PRP ADJ N\n",
    "* VP -> V PP | V NP\n",
    "* PP -> PREP PP | PREP NP\n",
    "* PRP -> my | your | her\n",
    "* DET -> a | the | this\n",
    "* ADJ -> favorite | worst | dang\n",
    "* N -> späti | cat | seminar | Toblerone\n",
    "* V -> is | has | smells\n",
    "* PREP -> out | of | like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Derivation\n",
    "\n",
    "Format for each step: State after rule application (index of rule applied)\n",
    "\n",
    "<.>\n",
    "\n",
    "S (1)\n",
    "\n",
    "NP VP (2)\n",
    "\n",
    "PRP ADJ N VP (3)\n",
    "\n",
    "PRP ADJ N V PP (4)\n",
    "\n",
    "PRP ADJ N V PREP PP (5)\n",
    "\n",
    "PRP ADJ N V PREP PREP NP (5)\n",
    "\n",
    "PRP ADJ N V PREP PREP N (3)\n",
    "\n",
    "My ADJ N V PREP PREP N (6)\n",
    "\n",
    "My favorite N V PREP PREP N (8)\n",
    "\n",
    "My favorite späti V PREP PREP N (9)\n",
    "\n",
    "My favorite späti is PREP PREP N (10)\n",
    "\n",
    "My favorite späti is out PREP N (11)\n",
    "\n",
    "My favorite späti is out of N (11)\n",
    "\n",
    "My favorite späti is out of Toblerone (9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing trees\n",
    "\n",
    "* **linearized** - cf. [Vinyals et al 2015](https://arxiv.org/pdf/1412.7449.pdf) (Hinton's group at Google)\n",
    "\n",
    "    (S (NP My favorite späti) (VP is (PP out (PP of (NP Toblerone)))) .)\n",
    "\n",
    "\n",
    "* **action sequence** (this paper)\n",
    "\n",
    "    Given \n",
    "\n",
    "    * *a* - a specific parsing algorithm (i.e. depth-first, left-to-right traversal),\n",
    "    * *x* - a sequence of symbols, and\n",
    "    * *y* - a parse tree,\n",
    "\n",
    "    there is a unique sequence of actions\n",
    "\n",
    "    \\begin{align}\n",
    "    a(x, y)\n",
    "    \\end{align}\n",
    "    \n",
    "    which can generate the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shift-reduce action sequence\n",
    "\n",
    "* *SHIFT* : In parsing - *shift* word from buffer onto stack\n",
    "    * $\\rightarrow$ *GEN(x)* : in generating - *generate* new terminal *x*\n",
    "* *NT(X)* : Open a nonterminal constituent of type X (i.e. NP, VP, ...)\n",
    "* *REDUCE* : Close an open nonterminal constituent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from get_oracle_gen import get_actions, get_tags_tokens_lowercase\n",
    "example = \"(S (NP (PRP My) (ADJ favorite) (N späti)) (VP (V is) (PP (PREP out) (PP (PREP of) (NP (N Toblerone))))) (. .))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "actions = get_actions(example)\n",
    "tags, tokens, lowercase = get_tags_tokens_lowercase(example)\n",
    "i, j = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"{}: {}\".format(i, actions[i]))\n",
    "i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"{}: {}\".format(j, tokens[j]))\n",
    "j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parser architecture\n",
    "\n",
    "Interim parse states are stored on the **stack**, which at any time *t* may contain:\n",
    "\n",
    "* open nonterminals\n",
    "* completed nonterminals\n",
    "* terminals\n",
    "\n",
    "The resulting sequences of actions and terminals are stored in the **action history** and **output buffer** respectively.\n",
    "\n",
    "(In discriminative parsing, instead of an output buffer, we have an *input buffer* to read in the given symbol sequence.)\n",
    "\n",
    "<img src=\"img/gen_derivation.png\",width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNG (Recurrent Neural Network Grammar)\n",
    "\n",
    "<!-- G = (N, Sig, P, S)-->\n",
    "\n",
    "<!--img src=\"img/rnng_grammar.png\",width=120,height=120-->\n",
    "\n",
    "\\begin{align}\n",
    "G = (N, \\Sigma, \\Theta)\n",
    "\\end{align}\n",
    "\n",
    "* N: set of nonterminals\n",
    "* $\\Sigma$: set of terminals (tokens: words, punctuation)\n",
    "* $\\Theta$: parameters learned by network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure valid parse trees, a constrained set of valid parser actions are defined at time step *t*:\n",
    "\n",
    "\\begin{align}\n",
    "A_G(S, n)\n",
    "\\end{align}\n",
    "\n",
    "* *$S$*: state of the stack\n",
    "* *n*: number of open nonterminals on the stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generative model\n",
    "\n",
    "Learns a joint probability distribution of sentences and action sequences:\n",
    "\n",
    "\\begin{align}\n",
    "p(x, y)& = \\prod_{t=1}^{|a(x,y)|}p(a_t|\\mathbf{a}_{\\lt{t}}) \\\\\n",
    "& = \\prod_{t=1}^{|a(x,y)|}\\frac{\\exp \\mathbf{r}_{at}^\\top \\mathbf{u}_t + b_{at}}{\\sum_{a' \\in A_G(S_t, n_t)} \\exp \\mathbf{r}_{a'}^\\top \\mathbf{u}_t + b_{a'}}\n",
    "\\end{align}\n",
    "\n",
    "* **$r_a$**: embedding of the action $a \\in (SHIFT, REDUCE, NT(X_1), ..., NT(X_n))$\n",
    "* **$u_t$**: embedding of the algorithm context at time *t*\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{u}_t = tanh(\\mathbf{Ws}_t + \\mathbf{c})\n",
    "\\end{align}\n",
    "\n",
    "* **$s_t$**: embedding of the stack at time *t*\n",
    "\n",
    "The original paper also incorporated embeddings of the output buffer - i.e. the sequence of words that had already been generated until time *t* - and the action history - the sequence of actions taken until time *t* - but [Kuncoro et al. 2017](http://www.aclweb.org/anthology/E17-1117) found that the RNNG performed best when trained on the stack embedding alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminative model\n",
    "\n",
    "Learns a conditional probability distribution of action sequences given a sentence.\n",
    "\n",
    "The equation is the same as above, with three changes:\n",
    "\n",
    "\\begin{align}\n",
    " p(x, y) & \\rightarrow p(y|x) \\\\\n",
    " A_G(S, n) & \\rightarrow A_D(B, S, n) \\\\\n",
    " \\mathbf{u}_t = tanh(\\mathbf{Ws}_t + \\mathbf{c}) & \\rightarrow tanh(\\mathbf{W}[\\mathbf{s}_t;\\mathbf{b}_t] + \\mathbf{c})\n",
    "\\end{align}\n",
    "\n",
    "* $p(y|x)$: conditional probability given the input sentence $x$\n",
    "* $A_D$: set of valid parser actions for the discriminative model\n",
    "    * $B$: input buffer containing the not-yet-parsed sequence of tokens in *x*\n",
    "* $b_t$: embedding of the input buffer state at time *t*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Representing the stack: recursive NN composition function\n",
    "\n",
    "As noted above, the stack can contain:\n",
    "\n",
    "* open nonterminals\n",
    "* completed nonterminals\n",
    "* terminals\n",
    "\n",
    "Terminals and open nonterminals can be treated as simple lookup parameters, so that's just a few more embeddings for the model to learn.\n",
    "\n",
    "Completed nonterminals result from REDUCE operations, and they represent trees:\n",
    "\n",
    "<img src=\"img/architecture_stack.png\",width=300>\n",
    "\n",
    "To obtain a representation of these trees, we need a **composition function** that can take sequences of arbitrary length.\n",
    "\n",
    "Dyer et al. solve this problem by throwing a bidirectional LSTM at it:\n",
    "\n",
    "<img src=\"img/stack_comp.png\",width=500>\n",
    "\n",
    "This gives us a **recursive neural network** approach to composition, as representations of terminals and subtrees are treated as equivalents in building the next tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generating words: class-factored softmax\n",
    "\n",
    "Hypothetically, the number of valid actions for the generative model to choose from would be:\n",
    "\n",
    "\\begin{align}\n",
    "N + \\Sigma + 1\n",
    "\\end{align}\n",
    "\n",
    "which becomes very daunting when dealing with a large vocabulary.\n",
    "\n",
    "But if word generation is one action, GEN/SHIFT, the set of valid actions is $N + 2$ :\n",
    "\n",
    "\\begin{align}\n",
    "a \\in (SHIFT, REDUCE, NT(X_1), ..., NT(X_n))\n",
    "\\end{align}\n",
    "\n",
    "When the parser predicts GEN/SHIFT, word generation is handled in a second step using **class-factored softmax** where words are sampled based on a predetermined subclass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from rnng import Vocab, TransitionParser, main\n",
    "import dynet as dy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model, tp, train, dev = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walk through one tree-building example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "724\n"
     ]
    }
   ],
   "source": [
    "which = random.randrange(len(train))\n",
    "print(which)\n",
    "t_inst = train[which]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# (FRAG (`` ``) (CC Or) (SBAR (IN if) (S  (NP (PRP they) ) (VP (VBP feel) (SBAR  (SBAR  (S  (NP (DT the) (NN wine) ) (VP (VBZ is) (VP (VBN overpriced) )))) (CC and) (SBAR  (S  (NP (PRP they) ) (VP (MD can) (VP (VB get) (NP  (NP (NN something) ) (ADJP (RB equally) (JJ good) )) (PP (IN for) (ADJP (JJR less) )))))))))) (. .) ('' '') )\n",
      "\n",
      "`` Or if they feel the wine is overpriced and they can get something equally good for less . ''\n",
      "\n",
      "NT(FRAG) SHIFT SHIFT NT(SBAR) SHIFT NT(S) NT(NP) SHIFT REDUCE NT(VP) SHIFT NT(SBAR) NT(SBAR) NT(S) NT(NP) SHIFT SHIFT REDUCE NT(VP) SHIFT NT(VP) SHIFT REDUCE REDUCE REDUCE REDUCE SHIFT NT(SBAR) NT(S) NT(NP) SHIFT REDUCE NT(VP) SHIFT NT(VP) SHIFT NT(NP) NT(NP) SHIFT REDUCE NT(ADJP) SHIFT SHIFT REDUCE REDUCE NT(PP) SHIFT NT(ADJP) SHIFT REDUCE REDUCE REDUCE REDUCE REDUCE REDUCE REDUCE REDUCE REDUCE REDUCE SHIFT SHIFT REDUCE\n"
     ]
    }
   ],
   "source": [
    "def print_train_inst(t_inst):\n",
    "  print(t_inst[0] + \"\\n\\n\" + \" \".join(t_inst[1]) + \"\\n\\n\" + \" \".join(t_inst[2]))\n",
    "  \n",
    "print_train_inst(t_inst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stack, params = tp.gen_setup(.3)\n",
    "terms, results, open_nts, losses = [], [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start loop here\n",
    "\n",
    "Executing each of the cells below in sequence will walk you through one parsing step for the network and expose intermediate states of the stack so you can inspect them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_actions = tp.get_valid_actions(stack, open_nts)\n",
    "#[tp.vocab_acts.i2w[action] for action in valid_actions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "action, loss = tp.get_action(stack, params, valid_actions, len(results), t_inst[2], .3)\n",
    "losses.append(loss) if loss else loss\n",
    "results.append(action)\n",
    "tp.vocab_acts.i2w[action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "term, nt_idx, loss = tp.do_action(stack, action, params, open_nts, len(terms), t_inst[1], .3)\n",
    "losses.append(loss) if loss else loss\n",
    "terms.append(term) if term else term\n",
    "open_nts.append(nt_idx) if nt_idx else nt_idx \n",
    "term, nt_idx, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stop loop here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train a model for one epoch - takes ~25 min on my machine\n",
    "tp.train(train, dy.SimpleSGDTrainer(model), dev, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate a sentence from the trained model\n",
    "tp.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save(\"1epoch.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "model, tp, train, dev = main()\n",
    "model.populate(\"1epoch.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.parameter_count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnng-notebook-tiFSsUcM",
   "language": "python",
   "name": "rnng-notebook-tifssucm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
